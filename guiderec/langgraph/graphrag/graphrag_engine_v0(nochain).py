from langchain.prompts.prompt import PromptTemplate
from langchain_core.messages import SystemMessage
from langchain_core.messages import HumanMessage

from langchain_google_genai import ChatGoogleGenerativeAI

from retry import retry
from timeit import default_timer as timer

from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm
from dotenv import load_dotenv, find_dotenv


from neo4j import GraphDatabase
from json import loads
import json

import os


load_dotenv()

os.environ["KYEONGCHAN_GEMINI_API_KEY"] = os.getenv("KYEONGCHAN_GEMINI_API_KEY") 
gemini_key = os.environ["KYEONGCHAN_GEMINI_API_KEY"]
os.environ["NEO4J_URI"] = os.getenv("NEO4J_URI")
os.environ["NEO4J_USERNAME"] =os.getenv("NEO4J_USERNAME")
os.environ["NEO4J_PASSWORD"] =os.getenv("NEO4J_PASSWORD")

neo4j_url = os.environ["NEO4J_URI"]
neo4j_user = os.environ["NEO4J_USERNAME"]
neo4j_password = os.environ["NEO4J_PASSWORD"] 

import google.generativeai as genai


# Load BGE-M3-Korean model
tokenizer = AutoTokenizer.from_pretrained("upskyy/bge-m3-korean")
model = AutoModel.from_pretrained("upskyy/bge-m3-korean")

# Define mean pooling function
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

# Function to get embeddings for a given text
def get_embedding(text):
    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors="pt")
    with torch.no_grad():
        model_output = model(**encoded_input)
    return mean_pooling(model_output, encoded_input["attention_mask"]).squeeze().tolist()



# ì œë¯¸ë‚˜ì´ API í‚¤ ì„¤ì •
genai.configure(api_key=gemini_key)
llm_model = genai.GenerativeModel(model_name='gemini-1.5-flash')

SYSTEM_PROMPT = """You are an expert in recommending restaurants.
* Create answers in Korean
* If the question is not about restaurants recommendation, please answer like this:
Sorry, I can only answer questions related to restaurants recommendation.
* Don't answer the same sentence repeatedly.

"""

PROMPT_TEMPLATE = """

{questions}

Here is the context in JSON format. This dataset contains information about restaurants that will be recommended to the user.


<context>
{context}
</context>

When recommending restaurants to a user related to a question, make sure to recommend at least five restaurants included in the context!
Create answers in Korean
Please add the following phrase at the end of your answer : 

The following is an example of a response when recommending a restaurants to a user :
Hello! My name is ğŸ˜ Agent ì˜ë„ë§›ìˆìˆ˜ë‹¤ , a restaurants recommendation chatbot that specializes in restaurants recommendations. I recommend restaurants based on GraphRAG.
Based on your questions, I'll recommend restaurants you might like.

ğŸ´ ìŒì‹ì  : (ì£¼)ì‹œë”ìŠ¤ì´ˆë°¥ì œì£¼ì—°ë™ì 
ğŸš© ì£¼ì†Œ : ì œì£¼ ì œì£¼ì‹œ ì—°ë™ 355-8ë²ˆì§€ 1ì¸µ
ğŸ“· ì‚¬ì§„ : https://lh5.googleusercontent.com/p/AF1QipPI4j5Ml2zbxvH86gKvyYaGl55jHtWYR-l7PcTU=w408-h306-k-no
ğŸŒŸ ë³„ì  : 4.5
âœ“ ìŒì‹ì  ì¶”ì²œ ì´ìœ  : ì§ˆë¬¸ì—ì„œ ì´ˆë°¥ì´ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì¶”ì²œí•˜ì˜€ìŠµë‹ˆë‹¤.

"""


PROMPT = PromptTemplate(
    input_variables=["questions","context"], template= PROMPT_TEMPLATE
)


def run_query(url, user, password, query, params):
    driver = GraphDatabase.driver(url, auth=(user, password))
    with driver.session() as session:
        # print(params)
        result = session.run(query, params)
        # print(result)
        return [record for record in result]

def vector_graph_qa(query):
    query_vector = get_embedding(query)
    # print(query_vector)
    url = neo4j_url
    user = neo4j_user
    password = neo4j_password
    params = {'queryVector':query_vector}
    cypher_query = """
    CALL db.index.vector.queryNodes('queryVector', 5, $queryVector)
    YIELD node AS doc, score
    match (doc)<-[s:HAS_REVIEW]-(store:STORE)
    RETURN store AS store_Info, 
        collect('The type of the store ' + store.MCT_NM + ' is ' + store.MCT_TYPE + 
        ', the rating of store is ' + store.rating + ' and the review of the store is ' + doc.text) AS StoreInfo,  
        score
    ORDER BY score DESC LIMIT 5
    """
    result = run_query(url, user, password, cypher_query, params)
    # print(result)
    return result



@retry(tries=5, delay=5)
def get_results(question):
    start = timer()
    try:
        df = vector_graph_qa(question)

        ans = PROMPT.format(questions=question, context=df)
        
        chat_llm = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=gemini_key,
                                    convert_system_message_to_human=True) 
        messages = chat_llm(
        [  
            SystemMessage(content="SYSTEM_PROMPT"),
            HumanMessage(content=ans)
        ]
        )
        result = messages.content
        r = {'context': df, 'result': result}


        return result
    finally:
        print('Response Generation Time : {}'.format(timer() - start))
        
print(get_results('ì œì£¼ì˜ ê³ ê¸°ì§‘ì„ ê°ˆë ¤ê³ í•˜ëŠ”ë° ì¶”ì²œí•´ì¤˜!'))



