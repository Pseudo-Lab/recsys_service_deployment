from llm_response.langgraph_graph_state import GraphState
from prompt.routing_and_intent_analysis import CASUAL_RESPONSE_PROMPT


def casual_response(llm, state: GraphState) -> dict:
    """ì¼ìƒì ì¸ ëŒ€í™”ì— ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•˜ê³  ë§›ì§‘ ì¶”ì²œìœ¼ë¡œ ìœ ë„í•©ë‹ˆë‹¤."""
    query = state["query"]
    prompt = CASUAL_RESPONSE_PROMPT.format(query=query)

    res = llm.invoke(prompt)
    content = res.content or "ì•ˆë…•! ğŸŠ ë‚˜ëŠ” ì œì£¼ ë§›ì§‘ ì¶”ì²œí•´ì£¼ëŠ” AIì•¼~ ì˜¤ëŠ˜ ë­ ë¨¹ê³  ì‹¶ì–´?"

    return {"final_answer": content}
