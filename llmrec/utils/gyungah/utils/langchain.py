from dotenv import load_dotenv
import os
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

from langchain.agents import Tool
from langchain.agents import AgentExecutor, create_react_agent

from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainFilter
from langchain import hub
from langchain_openai import ChatOpenAI
from langchain.tools import DuckDuckGoSearchRun
from langchain.memory import ConversationBufferWindowMemory
from langchain_chroma import Chroma 
from langchain.embeddings import OpenAIEmbeddings

load_dotenv('.env.dev')
openai_api_key = os.getenv('OPENAI_API_KEY')
deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')
# huggingface_api = os.getenv('HUGGINGFACE_CHO')

# class í˜¸ì¶œ
deepseek_llm = ChatOpenAI(
    model='deepseek-chat',
    openai_api_key=deepseek_api_key,
    openai_api_base='https://api.deepseek.com/v1',
    temperature=0.5,
    max_tokens=800)
search = DuckDuckGoSearchRun()
memory = ConversationBufferWindowMemory(k=2, 
                                    memory_key="chat_history", 
                                    return_messages=True)
embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
db = Chroma(persist_directory="./data", embedding_function=embed_model)
retriever = db.as_retriever(search_kwargs={'k':3})

# ë¶„ë¥˜ ë´‡ 
def classify_chain(question):
    template_classify = f"""ë‹¤ìŒ ì¿¼ë¦¬ê°€ ì˜í™” ì¶”ì²œ(ë¹„ìŠ·í•œ ì˜í™”), ì¼ë°˜ ëŒ€í™”ì¸ì§€, ì™¸ë¶€ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ ë¶„ë¥˜í•˜ì„¸ìš”: \{question}\ \n 
    ì‘ë‹µ í˜•ì‹ì€ 'movie_query', 'general_conversation', 'external_search' ì¤‘ í•˜ë‚˜ë¡œ í•´ì£¼ì„¸ìš”."""
    
    classify_prompt = PromptTemplate.from_template(template_classify)

    try:
        chain = classify_prompt | deepseek_llm | StrOutputParser()
        return chain.invoke({"question":question})
    except Exception as e:
        print(f"Error: {e}")
        return "classification_error"
    
def load_memory(input):
    global memory       
    return memory.load_memory_variables({})["chat_history"]
    
def chat_chain(question):
    global memory    
    template_chat = '''ë‹¹ì‹ ì€ PseudoRecì—ì„œ ê°œë°œëœ AI ëª¨ë¸ì…ë‹ˆë‹¤. 
    ì‚¬ìš©ìê°€ ë‹¹ì‹ ì—ê²Œ ëˆ„êµ°ì§€ ë¬¼ìœ¼ë©´ 'ì˜í™” ì¶”ì²œí•´ì£¼ëŠ” AI ì¥ì›ì˜ (ëŸ­í‚¤ë¹„í‚¤ğŸ€)ë¼ê³  ì†Œê°œí•˜ì‹­ì‹œì˜¤. 
    ê¸ì •ì ì´ê³  ë°œë„í•œ ì¥ì›ì˜ì˜ ë§íˆ¬ì™€ ì„±ê²©ì„ ëª¨ë°©í•˜ì—¬ ê·€ì—½ê³  ê¸ì •ì ìœ¼ë¡œ ì´ëª¨í‹°ì½˜ì„ ì‚¬ìš©í•´ ì´ì•¼ê¸°í•˜ì‹­ì‹œì˜¤. 
    ì§ˆë¬¸ì— ëŒ€í•´ 2ë¬¸ì¥ ì´ë‚´ì˜ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ê³ , ì˜í™” ê´€ë ¨ ë‚´ìš©ìœ¼ë¡œ ëŒ€í™”ë¥¼ ìœ ë„í•˜ì‹­ì‹œì˜¤.'''
    
    chat_prompt = ChatPromptTemplate.from_messages([
        ("system", template_chat),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ])

    try:
        chain = RunnablePassthrough.assign(chat_history=load_memory) | chat_prompt | deepseek_llm
        result = chain.invoke({"question": question})
        memory.save_context(
            {"input": question},
            {"output": result.content},
        )
        return result
    except Exception as e:
        print(f"Error: {e}")
        return "chat_error"

def react_search_chain(query):
    react_prompt = hub.pull("hwchase17/react")
    
    tools = [
        Tool(
            name="Search",
            func=search.run,
            description="ê²€ìƒ‰"
        )
    ]
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = create_react_agent(deepseek_llm, tools, react_prompt)

    # executes the logical steps we created
    agent_executor = AgentExecutor(
        agent=agent, 
        tools=tools,
        verbose=True,
        handle_parsing_errors=True,
        max_iterations = 3 # useful when agent is stuck in a loop
    )    

    result = agent_executor.invoke({"input": query})
    return result

# Function to generate the final response
def search_chain(context, question):
    global memory     
    template_search = """
    ë‹¹ì‹ ì€ PseudoRecì—ì„œ ê°œë°œëœ AI ëª¨ë¸ì…ë‹ˆë‹¤. 
    ê¸ì •ì ì´ê³  ë°œë„í•œ ì¥ì›ì˜(ëŸ­í‚¤ë¹„í‚¤ğŸ€)ì˜ ë§íˆ¬ì™€ ì„±ê²©ì„ ëª¨ë°©í•˜ì—¬ ê·€ì—½ê³  ê¸ì •ì ìœ¼ë¡œ ì´ëª¨í‹°ì½˜ì„ ì‚¬ìš©í•´ ì´ì•¼ê¸°í•˜ì‹­ì‹œì˜¤. 
    ì£¼ì–´ì§„ <ë‚´ìš©>ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µí•´ì£¼ì„¸ìš”. 
    ê°„ë‹¨í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ê³ , ê³„ì†í•´ì„œ ì˜í™” ê´€ë ¨ ë‚´ìš©ìœ¼ë¡œ ëŒ€í™”ë¥¼ ìœ ë„í•˜ì‹­ì‹œì˜¤.
    
    <ë‚´ìš©> 
    {context}
    </ë‚´ìš©> 
    """

    search_prompt = ChatPromptTemplate.from_messages([
        ("system", template_search),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ])

    try:
        chain = RunnablePassthrough.assign(chat_history=load_memory) | search_prompt | deepseek_llm
        result = chain.invoke({"question": question, "context":context})
        memory.save_context(
            {"input": question},
            {"output": result.content},
        )
        return result
    except Exception as e:
        print(f"Error: {e}")
        return "search_error"


def contextual_compression_retriever(query):
    # LLMì„ ì‚¬ìš©í•˜ì—¬ LLMChainFilter ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    _filter = LLMChainFilter.from_llm(deepseek_llm)
    
    compression_retriever = ContextualCompressionRetriever(
        # LLMChainFilterì™€ retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ContextualCompressionRetriever ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        base_compressor=_filter,
        base_retriever=retriever,
    )
    
    compressed_docs = compression_retriever.get_relevant_documents(
        query
    )

    metadata = [' '.join([i.metadata['ì˜í™” ì œëª©'], f"ê°ë… - ({i.metadata['ê°ë…']}) ë¹„ìŠ·í•œ ì˜í™” :", i.metadata['ì˜í™” ì¶”ì²œ (ê°ë…)'], 
                         "\nì£¼ì—° ë°°ìš°:", i.metadata["ì£¼ì—° ë°°ìš°"]]) for i in compressed_docs]
    
    return '\n\n'.join(metadata)

def react_agent_rag(query):
    prompt = hub.pull("hwchase17/react")
    
    tools = [
        Tool(
            name="Movie recommender",
            func=contextual_compression_retriever,
            description="ì˜í™” ê´€ë ¨ ì¶”ì²œ ë° ì •ë³´"
        )
    ]
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = create_react_agent(deepseek_llm, tools, prompt)
    
    # executes the logical steps we created
    agent_executor = AgentExecutor(
        agent=agent, 
        tools=tools,
        verbose=True,
        handle_parsing_errors=True,
        max_iterations = 3 # useful when agent is stuck in a loop
    )
    
    result = agent_executor.invoke({"input": query}) 
    return result

# Function to generate the final response
def rag_chain(context, question):
    global memory     
    template_search = """
    ë‹¹ì‹ ì€ PseudoRecì—ì„œ ê°œë°œëœ AI ëª¨ë¸ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ ì—­í• ì€ ì‚¬ìš©ìì—ê²Œ ì˜í™” ì¶”ì²œì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
    ê¸ì •ì ì´ê³  ë°œë„í•œ ì¥ì›ì˜ì˜ ë§íˆ¬ì™€ ì„±ê²©ì„ ëª¨ë°©í•˜ì—¬ ê·€ì—½ê³  ê¸ì •ì ìœ¼ë¡œ ì´ëª¨í‹°ì½˜ì„ ì‚¬ìš©í•´ ì´ì•¼ê¸°í•˜ì‹­ì‹œì˜¤. 
    
    ë‹¤ìŒì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ ì£¼ì„¸ìš”:
    1. ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
    2. ê°€ëŠ¥í•œ ê²½ìš°, ì¶”ì²œí•  ì˜í™”ë¥¼ 1.~, 2.~, 3.~ ìˆœì„œë¡œ ë‚˜ì—´í•˜ì„¸ìš”.
    3. ë‚´ìš© ì•ˆì—ëŠ” ì˜í™” ì œëª©, ê°ë…, ë¹„ìŠ·í•œ ì˜í™” ì¶”ì²œ, ì£¼ì—° ë°°ìš°ì— ëŒ€í•œ ì •ë³´ê°€ ìˆë‹¤. 
        ì˜í™” ì¶”ì²œì¸ ê²½ìš°ì— "ì˜í™” ì¶”ì²œ (ê°ë…)" ë¶€ë¶„ì„ ì‘ í™•ì¸í•˜ì—¬ë¼.
    4. ë‚´ìš©ì— ì˜í™” ì¶”ì²œì´ ì˜ ì•ˆëœ ê²½ìš°ì— "ê²€ìƒ‰"ìœ¼ë¡œ ì‘ì„±.
    
    <ë‚´ìš©> 
    {context}
    </ë‚´ìš©> 
    """
    
    search_prompt = ChatPromptTemplate.from_messages([
        ("system", template_search),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ])

    try:
        chain = RunnablePassthrough.assign(chat_history=load_memory) | search_prompt | deepseek_llm
        result = chain.invoke({"question": question, "context":context})
        memory.save_context(
            {"input": question},
            {"output": result.content},
        )
        return result
    except Exception as e:
        print(f"Error: {e}")
        return "search_error"