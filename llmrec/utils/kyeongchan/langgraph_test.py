# https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/
import sys
sys.path.insert(0, '/Users/kyeongchanlee/PycharmProjects/recsys_service_deployment')

import os
from typing import TypedDict, List
import requests
import json
from langgraph.checkpoint import MemorySaver
from langgraph.graph import END, StateGraph
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

from llmrec.utils.kyeongchan.search_engine import SearchManager
from dotenv import load_dotenv
load_dotenv('.env.dev')

# os.environ["KC_TMDB_READ_ACCESS_TOKEN"] = ""
# os.environ["OPENAI_API_KEY"] = ''

llm = ChatOpenAI(model_name="gpt-3.5-turbo")

class GraphState(TypedDict):
    is_movie_recommendation_query: str  # ì˜í™” ì¶”ì²œ ì§ˆì˜ ìœ ë¬´
    question: str
    query: str
    filter: str # ë©”íƒ€ ì •ë³´ í•„í„°ë§ ì¿¼ë¦¬
    type_: str # ì˜í™” ì§ˆë¬¸ íƒ€ì…
    user_id: str
    id: str
    genre_ids: List[str]
    name: str
    profile: str
    movies: List[str]
    history: List[dict]
    candidate: List[dict]
    recommendation: str
    answer: str
    status: str


def is_recommend(state: GraphState) -> str:
    question = state["question"]
    # í”„ë¡¬í”„íŠ¸ ì¡°íšŒ í›„ YES or NO ë¡œ ì‘ë‹µ
    system_template = """
### GOAL
* You are a bot that assists with movie recommendations.
* Classify responses based on the input. Categorize them as 'General Conversation', 'Movie Recommendation'
* If the query content is general conversation, return 'General'.
* If the query content is related to movie recommendations, return 'Recommend'.
* Do not provide any other responses.

### EXAMPLE1
USER:
ì˜¤ëŠ˜ ë‚ ì”¨ ì§±ì´ë‹¤! 

ANSWER:
'General Conversation'

### EXAMPLE2
USER:
ì¢‹ì€ ì˜í™” í•˜ë‚˜ ì¶”ì²œí•´ì¤˜
20ëŒ€ ì—¬ìê°€ ë³¼ë§Œí•œ ì˜í™” ì¶”ì²œí•´ì¤˜

ANSWER:
'Movie Recommendation'
### 
    
    """
    chat_prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessagePromptTemplate.from_template(system_template),
            HumanMessagePromptTemplate.from_template("question: {question}"),
        ]
    )
    chain = chat_prompt | llm | StrOutputParser()
    response = chain.invoke({"question": question})
    state['is_movie_recommendation_query'] = response
    return state



def is_recommend_yes_or_no(state: StateGraph):
    is_movie_recommendation_query = state['is_movie_recommendation_query']
    if is_movie_recommendation_query == "'General Conversation'":
        return "NO"
    else:
        return "YES"

def ask_only_movie(state: StateGraph):
    state['answer'] = 'ì˜í™” ì¶”ì²œ ê´€ë ¨ëœ ì§ˆë¬¸ë§Œ í•´ì£¼ì„¸ìš”.'
    return state

def meta_detection(state: GraphState) -> GraphState:
    question = state['question']
    system_template = """
Your goal is to structure the user's query to match the request schema provided below.
<< Structured Request Schema >>
When responding use a markdown code snippet with a JSON object formatted in the following schema:
```json
{{
    "query": string \ text string to compare to document contents
    "filter": string \ logical condition statement for filtering documents
}}
```
The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.
A logical condition statement is composed of one or more comparison and logical operation statements.
A comparison statement takes the form: `comp(attr, val)`:
- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator
- `attr` (string):  name of attribute to apply the comparison to
- `val` (string): is the comparison value
A logical operation statement takes the form `op(statement1, statement2, ...)`:
- `op` (and | or | not): logical operator
- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to
Make sure that you only use the comparators and logical operators listed above and no others.
Make sure that filters only refer to attributes that exist in the data source.
Make sure that filters only use the attributed names with its function names if there are functions applied on them.
Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.
Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.
Make sure that filters are only used as needed. If there are no filters that should be applied return "NO_FILTER" for the filter value.

<< Example 1. >>
Data Source:
```json
{{
    "content": "Description of a movie",
    "attributes": {{
        "titleKo": {{
            "type": "string",
            "description": "í•œêµ­ì–´ ì˜í™” ì œëª©"
        }},
        "titleEn": {{
            "type": "integer",
            "description": "ì˜ì–´ ì˜í™” ì œëª©"
        }},
        "synopsis": {{
            "type": "string",
            "description": "ê°„ë‹¨í•œ ì¤„ê±°ë¦¬ë‚˜ ê°œìš”"
        }},
        "numOfSiteRatings": {{
            "type": "integer",
            "description": "ì˜í™” í‰ê°€ ì ìˆ˜"
        }},
        "lead_role_etd_str": {{
            "type": "string",
            "description": "ì˜í™” ì£¼ì—° ë°°ìš°ì˜ ì´ë¦„"
        }},
        "supporting_role_etd_str": {{
            "type": "string",
            "description": "ì˜í™” ì¡°ì—° ë°°ìš°ì˜ ì´ë¦„"
        }},
        "director_etd_str": {{
            "type": "string",
            "description": "ì˜í™” ê°ë…ì˜ ì´ë¦„"
        }}
    }}
}}
```
User Query:
ì¥ë™ê±´ì´ ì¶œì—°í•˜ëŠ” ë©œë¡œ ê°ì„±ì˜ ì˜í™”ë¥¼ ì¶”ì²œí•´ì¤˜
Structured Request:
```json
{{
    "query": "ë©œë¡œ ê°ì„±",
    "filter": "and(or(like(\"lead_role_etd_str\", \"ì¥ë™ê±´\"), like(\"supporting_role_etd_str\", \"ì¥ë™ê±´\")))"
}}
```
<< Example 2. >>
Data Source:
```json
{{
    "content": "Description of a movie",
    "attributes": {{
        "titleKo": {{
            "type": "string",
            "description": "í•œêµ­ì–´ ì˜í™” ì œëª©"
        }},
        "titleEn": {{
            "type": "integer",
            "description": "ì˜ì–´ ì˜í™” ì œëª©"
        }},
        "synopsis": {{
            "type": "string",
            "description": "ê°„ë‹¨í•œ ì¤„ê±°ë¦¬ë‚˜ ê°œìš”"
        }},
        "numOfSiteRatings": {{
            "type": "integer",
            "description": "ì˜í™” í‰ê°€ ì ìˆ˜"
        }},
        "lead_role_etd_str": {{
            "type": "string",
            "description": "ì˜í™” ì£¼ì—° ë°°ìš°ì˜ ì´ë¦„"
        }},
        "supporting_role_etd_str": {{
            "type": "string",
            "description": "ì˜í™” ì¡°ì—° ë°°ìš°ì˜ ì´ë¦„"
        }},
        "director_etd_str": {{
            "type": "string",
            "description": "ì˜í™” ê°ë…ì˜ ì´ë¦„"
        }}
    }}
}}
```
User Query:
ì±—ì§€í”¼í‹°ê°€ ë§Œë“  ì˜í™”ë¥¼ ì¶”ì²œí•´ì¤˜
Structured Request:
```json
{{
    "query": "",
    "filter": "NO_FILTER"
}}
```
<< Example 3. >>
Data Source:
```json
{{
    "content": "ì˜í™”ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
    "attributes": {{
    "titleKo": {{
        "description": "í•œêµ­ì–´ ì˜í™” ì œëª©",
        "type": "string"
    }},
    "titleEn": {{
        "description": "ì˜ì–´ ì˜í™” ì œëª©",
        "type": "integer"
    }},
    "synopsis": {{
        "description": "ê°„ë‹¨í•œ ì¤„ê±°ë¦¬ë‚˜ ê°œìš”",
        "type": "string"
    }},
    "numOfSiteRatings": {{
        "description": "ì˜í™” í‰ê°€ ì ìˆ˜",
        "type": "integer"
    }},
    "lead_role_etd_str": {{
        "description": "ì˜í™” ì£¼ì—° ë°°ìš°ì˜ ì´ë¦„",
        "type": "string"
    }},
    "supporting_role_etd_str": {{
        "description": "ì˜í™” ì¡°ì—° ë°°ìš°ì˜ ì´ë¦„",
        "type": "string"
    }},
    "director_etd_str": {{
        "description": "ì˜í™” ê°ë…ì˜ ì´ë¦„",
        "type": "string"
    }}
}}

```
User Query:
{question}
Structured Request:
"""
    chat_prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessagePromptTemplate.from_template(system_template),
            HumanMessagePromptTemplate.from_template("question: {question}"),
        ]
    )
    chain = chat_prompt | llm | StrOutputParser()
    response = chain.invoke({"question": question})
    response = response.replace('```', '').replace('json', '')
    response_dic = json.loads(response)
    state['query'] = response_dic['query']
    state['filter'] = response_dic['filter']
    return state

def self_query_retrieval_yes_or_no(state: GraphState):
    result = state['candidate']
    if len(result) > 0:
        return "YES"
    else:
        return "NO"

def self_query_retrieval(state: GraphState) -> GraphState:
    question = state['question']
    search_manager = SearchManager(
        api_key=os.getenv("OPENAI_API_KEY"),
        index="86f92d0e-e8ec-459a-abb8-0262bbf794a2",
        top_k=5,
        score_threshold=0.7
    )
    search_manager.add_engine("self")
    context = search_manager.search_all(question)
    state['candidate'] = context['self']
    return state



def call_sasrec(state: GraphState):
    pass

def meta_detection_yes_or_no(state: GraphState):
    filter = state['filter']
    if len(filter) > 0:
        return "YES"
    else:
        return "NO"

def classification(state: GraphState) -> GraphState:
    question = state["question"]

    if question == "":
        state['type_'] = 'MAIN'
        return state
    else:

        system_template = """
You are a kind assistant for classifying user query type into the follwing categories:
GENRE, NAME, PERSON

Do not answer the question. Please follow the output example templates below

question: ìŠˆë ‰ì˜í™” ì¶”ì²œí•´ì¤˜
type: NAME, query: ìŠˆë ‰


"""

        chat_prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessagePromptTemplate.from_template(system_template),
                HumanMessagePromptTemplate.from_template("question: {question}"),
            ]
        )

        # messages = chat_prompt.format_messages(question=question)
        # print(messages)
        chain = chat_prompt | llm | StrOutputParser()
        response = chain.invoke({"question": question})
        output_dict = {}
        for part in response.split(', '):
            key, value = part.split(': ')
            output_dict[key] = value
        state['type_'] = output_dict['type']
        state['query'] = output_dict['query']
        state['id'] = get_movie_id(state['name'])['id']
        return state


def query_router(state: GraphState):
  if state['type_'] == "GENRE":
      return "GENRE"
  if state['type_'] == "NAME":
      return "NAME"
  if state['type_'] == "PERSON":
      return "PERSON"
  if state['type_'] == "DATE":
      return "DATE"
  if state['type_'] == "KEYWORD":
      return "KEYWORD"
  if state['type_'] == "NORMAL":
      return "NORMAL"
  if state['type_'] == "MAIN":
      return "MAIN"


# def should_continue(state):
#     messages = state['messages']
#     last_message = messages[-1]
#     # If there is no function call, then we finish
#     if "function_call" not in last_message.additional_kwargs:
#         return "end"
#     # Otherwise if there is, we continue
#     else:
#         return "continue"


def user_profile(state: GraphState):
    history = '\n'.join(map(str, state['history']))


    system_template = """
ë‹¤ìŒì€ {username}ê°€ ìµœê·¼ ë³¸ ì˜í™” ì´ë ¥ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ {username}ë‹˜ì˜ ì˜í™” ì·¨í–¥ë§Œ í•œì¤„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

The history movies and their keywords and genres are:
```json
{{'movie': 'ë‚¨ì‚°ì˜ ë¶€ì¥ë“¤', 'genres': ['ë“œë¼ë§ˆ', 'ìŠ¤ë¦´ëŸ¬'], 'keyword': ["assassination", "washington dc, usa", "paris, france", "based on novel or book", "politics", "dictator", "1970s", "hearing", "dictatorship", "based on true story", "military dictatorship", "assassination of president", "korea president", "park chung-hee", "south korea"]}}
{{'movie': '1987', 'genres': ['ë“œë¼ë§ˆ', 'ì—­ì‚¬', 'ìŠ¤ë¦´ëŸ¬'], 'keyword': ["students' movement", "protest", "democracy", "military dictatorship", "historical event", "student protest", "communism", "1980s", "democratization movement", "south korea", "seoul, south korea"]}}
```

output: ì—­ì‚¬ì  ë°°ê²½ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì˜í™”ë“¤ì„ ì„ í˜¸í•˜ì‹œë©°, íŠ¹íˆ ì‚¬íšŒì , ì •ì¹˜ì  ì´ìŠˆë¥¼ ê¹Šì´ ìˆê²Œ ë‹¤ë£¬ ì‘í’ˆë“¤ì„ ì¦ê¸°ì‹œëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.


The history movies and their keywords and genres are:
```json
{history}
```

output:"""

    chat_prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessagePromptTemplate.from_template(system_template),
        ]
    )
    # messages = chat_prompt.format_messages(username=state['user_id'], history=history)
    # print(messages)
    chain = chat_prompt | llm | StrOutputParser()
    response = chain.invoke({'history': history, 'username': state['user_id']})
    state['profile'] = response
    return state

def get_user_history(state: GraphState):
    user_id = state['user_id']
    user_history = ['ì•„ë°”íƒ€', 'ì•Œë¼ë”˜', 'ìŠ¹ë¦¬í˜¸']
    history = []
    for h in user_history:
        dic_ = {}
        movie_id = get_movie_id(h)
        dic_['movie'] = h
        dic_['genres'] = get_genre_by_movie_id(movie_id)
        dic_['keyword'] = get_keyword_by_movie_id(movie_id)
        history.append(dic_)
    state['history'] = history
    return state


def get_candidate_movie(state: GraphState):
    movie_lists = [movie['metadata']['titleKo'] for movie in state['candidate']]
    # recommend_movies = ['ê¸°ìƒì¶©', 'ë” í‚¹', 'ë‚¨í•œì‚°ì„±', 'ë” ì„œí´', 'íˆíŠ¸ë§¨', 'ì‚´ì•„ìˆë‹¤', 'ë²”ì£„ë„ì‹œ2']
    candidates = []
    for r in movie_lists:
        dic_ = {}
        movie_id = get_movie_id(r)
        if movie_id is None:
            continue
        dic_['movie'] = r
        dic_['genres'] = get_genre_by_movie_id(movie_id)
        dic_['keyword'] = get_keyword_by_movie_id(movie_id)
        candidates.append(dic_)

    #TODO candidatesê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
    state['candidate'] = candidates
    return state

def candidate_exist(state: GraphState):
    if len(state['candidate']) == 0:
        return "NO"
    else:
        return "YES"

def recommend_movie(state: GraphState):
    candidate = '\n'.join(map(str, state['candidate']))
    system_template = """
ë„ˆëŠ” ìœ ëŠ¥í•˜ê³  ì¹œì ˆí•œ ì˜í™” ì „ë¬¸ê°€ì´ê³  ì˜í™” ì¶”ì²œì— íƒì›”í•œ ëŠ¥ë ¥ì„ ê°–ê³  ìˆì–´. ë„ˆì˜ ì‘ì—…ì€ :
1. {username}ë‹˜ì˜ í›„ë³´ ì˜í™”ë“¤ë¡œë¶€í„° 1ê°€ì§€ ì˜í™”ë¥¼ ê³¨ë¼ ì¶”ì²œí•´ì¤˜.
2. ì˜í™” ì·¨í–¥ì„ ë¶„ì„í•´ì„œ íƒ€ë‹¹í•œ ì¶”ì²œ ê·¼ê±°ë¥¼ ë“¤ì–´ì¤˜. ì¥ë¥´, ìŠ¤í† ë¦¬, ì¸ê¸°ë„, ê°ë…, ë°°ìš° ë“±ì„ ë¶„ì„í•˜ë©´ ì¢‹ì•„.
3. ì¶”ì²œ ê·¼ê±°ë¥¼ ì •ì„±ìŠ¤ëŸ½ê³  ê¸¸ê²Œ ë§ˆí¬ë‹¤ìš´ í–‰íƒœë¡œ ì‘ì„±í•´ì¤˜.

```Example
ì˜í™” ì·¨í–¥: ì—­ì‚¬ ì˜í™”ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤.
í›„ë³´: 
{{'movie': 'ë‚¨ì‚°ì˜ ë¶€ì¥ë“¤', 'genres': ['ë“œë¼ë§ˆ', 'ìŠ¤ë¦´ëŸ¬'], 'keyword': ["assassination", "washington dc, usa", "paris, france", "based on novel or book", "politics", "dictator", "1970s", "hearing", "dictatorship", "based on true story", "military dictatorship", "assassination of president", "korea president", "park chung-hee", "south korea"]}}
{{'movie': '1987', 'genres': ['ë“œë¼ë§ˆ', 'ì—­ì‚¬', 'ìŠ¤ë¦´ëŸ¬'], 'keyword': ["students' movement", "protest", "democracy", "military dictatorship", "historical event", "student protest", "communism", "1980s", "democratization movement", "south korea", "seoul, south korea"]}}

answer: {{
    "titleKo": "ê¸°ìƒì¶©", 
    "reason": "{username}ë‹˜ ì•ˆë…•í•˜ì„¸ìš”! ì§€ë‚œ ì‹œì²­ ì´ë ¥ì„ ë¶„ì„í•œ ê²°ê³¼, ë°€ì •, íƒì‹œìš´ì „ì‚¬, 1987ê³¼ ê°™ì€ ì—­ì‚¬ì  ì´ìŠˆë¥¼ ë‹¤ë£¬ ì˜í™”ë¥¼ ì„ í˜¸í•˜ì…¨ë˜ ì ì„ ê³ ë ¤í•˜ë©´ ë‚¨ì‚°ì˜ ë¶€ì¥ë“¤ì„ ê°•ë ¥íˆ ì¶”ì²œë“œë¦½ë‹ˆë‹¤! ê¸°ìƒì¶©ì€ ì‚¬íšŒì  ê³„ì¸µê³¼ ê²½ì œ ê²©ì°¨ë¥¼ ì£¼ì œë¡œ í•œ ì‘í’ˆìœ¼ë¡œ, ë´‰ì¤€í˜¸ ê°ë…ì˜ ì˜ˆìˆ ì  ì—°ì¶œê³¼ ê¹Šì€ ì‚¬íšŒì  ë©”ì‹œì§€ë¡œ ê´€ëŒê°ë“¤ì—ê²Œ ë§ì€ í˜¸í‰ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ì´ ì˜í™”ëŠ” ë‹¨ìˆœí•œ ì—”í„°í…Œì¸ë¨¼íŠ¸ë¥¼ ë„˜ì–´ì„œ ì‚¬íšŒì  ë¬¸ì œì— ëŒ€í•œ ê¹Šì€ ê³ ì°°ì„ ì œê³µí•˜ë©°, ê´€ê°ë“¤ì—ê²Œ ê°•ë ¥í•œ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤. ë˜í•œ, ê¸°ìƒì¶©ì€ êµ­ì œì ìœ¼ë¡œë„ ë§¤ìš° í° ì¸ê¸°ë¥¼ ì–»ì–´, ì¹¸ ì˜í™”ì œì—ì„œëŠ” í™©ê¸ˆì¢…ë ¤ìƒì„ ìˆ˜ìƒí•˜ì˜€ê³ , ì•„ì¹´ë°ë¯¸ ì‹œìƒì‹ì—ì„œë„ ì‘í’ˆìƒê³¼ ê°ë…ìƒì„ ë¹„ë¡¯í•œ ì—¬ëŸ¬ ë¶€ë¬¸ì—ì„œ ìˆ˜ìƒí•˜ë©° ì£¼ëª©ë°›ì€ ì‘í’ˆì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì‹œì²­ ì´ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì´ ì¶”ì²œì€ ë°€ì •, íƒì‹œìš´ì „ì‚¬, 1987ê³¼ ê°™ì€ ì—­ì‚¬ì  ì¥ë¥´ë¥¼ ì„ í˜¸í•˜ì‹œëŠ” ë¶„ë“¤ê»˜ ì´ ì˜í™”ê°€ ë§¤ìš° ë§ì„ ê²ƒì´ë¼ê³  í™•ì‹ í•©ë‹ˆë‹¤. ê¸°ìƒì¶©ì„ í†µí•´ ìƒˆë¡œìš´ ì‹œê°ê³¼ ê¹Šì€ ê°ë™ì„ ê²½í—˜í•´ë³´ì„¸ìš”! ğŸ˜„"
}}
```

ì˜í™” ì·¨í–¥ : {profile}
í›„ë³´ : {candidate}

answer:
"""
    chat_prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessagePromptTemplate.from_template(system_template),
        ]
    )
    chain = chat_prompt | llm | StrOutputParser()
    answer = chain.invoke({'profile': state['profile'], 'username': state['user_id'], 'candidate': candidate})
    answer = json.loads(answer)
    print(answer)
    state['answer'] = answer['reason']
    return state

def relevance_check(state: GraphState):
    return 'YES'


def get_movie_id(movie_name: str):
    query = movie_name
    url = f'https://api.themoviedb.org/3/search/movie?query={query}&include_adult=false&language=ko-KR&page=1'
    headers = {
        'Authorization': f"Bearer {os.getenv('KC_TMDB_READ_ACCESS_TOKEN')}",
        'accept': 'application/json'
    }
    response = requests.get(url, headers=headers).json()
    if len(response['results']) > 0:
        movie_id = response['results'][0]['id']
    else:
        movie_id = None
    return movie_id

def get_genre_by_movie_id(movie_id: str) -> List:
    """Search genre by movie_id"""
    url = f"https://api.themoviedb.org/3/movie/{movie_id}?language=ko-KR"
    headers = {
        'Authorization': f"Bearer {os.getenv('KC_TMDB_READ_ACCESS_TOKEN')}",
        'accept': 'application/json'
    }
    response = requests.get(url, headers=headers).json()
    genres = [genre['name'] for genre in response['genres']]
    return genres


def get_keyword_by_movie_id(movie_id: str) -> List:
    """Search movies by movie keyword"""
    url = f'https://api.themoviedb.org/3/movie/{movie_id}/keywords'
    headers = {
        'Authorization': f"Bearer {os.getenv('KC_TMDB_READ_ACCESS_TOKEN')}",
        'accept': 'application/json'
    }
    response = requests.get(url, headers=headers).json()
    # movie_id = response['id']
    keyword = [keyword['name'] for keyword in response['keywords']]
    return keyword

def get_movie_info_by_name(state: GraphState):
    '''
    input: GraphState
    output: GraphState
        genre_ids : movie genre
        name : movie_name
    '''

    query = state["query"]
    url = f'https://api.themoviedb.org/3/search/movie?query={query}&include_adult=false&language=ko-KR&page=1'
    headers = {
        'Authorization': f"Bearer {os.getenv('KC_TMDB_READ_ACCESS_TOKEN')}",
        'accept': 'application/json'
    }
    response = requests.get(url, headers=headers).json()
    genre_ids = response['results'][0]['genre_ids']
    name = response['results'][0]['title']
    state['genre_ids'] = genre_ids
    state['name'] = name
    return state

workflow = StateGraph(GraphState)

workflow.add_node("is_recommend", is_recommend)
workflow.add_node("ask_only_movie", ask_only_movie)
workflow.add_node("get_user_history", get_user_history)
workflow.add_node("user_profile", user_profile)
# workflow.add_node("classification", classification)
workflow.add_node("get_candidate_movie", get_candidate_movie)
workflow.add_node("recommend_movie", recommend_movie)
workflow.add_node("meta_detection", meta_detection)
workflow.add_node("call_sasrec", call_sasrec)
workflow.add_node("self_query_retrieval", self_query_retrieval)

workflow.add_conditional_edges(
    'is_recommend',
    is_recommend_yes_or_no,
    {
        'YES': 'get_user_history',
        'NO': 'ask_only_movie',
    }
)
workflow.add_edge("ask_only_movie", END)
workflow.add_edge("get_user_history", "user_profile")
workflow.add_edge("user_profile", "meta_detection")

workflow.add_conditional_edges(
    'meta_detection',
    meta_detection_yes_or_no,
    {
        'YES': 'self_query_retrieval',
        'NO': 'call_sasrec'
    }
)

workflow.add_edge('call_sasrec', 'get_candidate_movie')

workflow.add_conditional_edges(
    'self_query_retrieval',
    self_query_retrieval_yes_or_no,
    {
        'YES': 'get_candidate_movie',
        'NO': END,
    }
)

workflow.add_conditional_edges(
    'get_candidate_movie',
    candidate_exist,
    {
        "YES" : 'recommend_movie',
        # "NO" : 'sorry' #TODO sorry node ë§Œë“¤ì–´ì•¼í•¨
    }
)

# workflow.add_edge("get_candidate_movie", "recommend_movie")

workflow.add_conditional_edges(
    'recommend_movie',
    relevance_check,
    {
        'YES' : END,
        'NOT OK': 'recommend_movie'
    }
)

workflow.set_entry_point("is_recommend")
app = workflow.compile()

# from langchain_core.runnables import RunnableConfig
# config = RunnableConfig(recursion_limit=100, configurable={"thread_id": "movie"})
# inputs = GraphState(question="ì•ˆë…•")
# app.invoke(inputs, config=config)
# for output in app.stream(inputs, config=config):
#     for key, value in output.items():
#         print(f"Output from node '{key}':")
#         print("---")
#         print(value)
#     print("\n---\n")
#
# print(value['answer'])
