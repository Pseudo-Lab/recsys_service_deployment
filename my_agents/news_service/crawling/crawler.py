import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def make_driver():
    """í—¤ë“œë¦¬ìŠ¤ í¬ë¡¬ ë“œë¼ì´ë²„ ìƒì„± (Selenium 4.6+ ìë™ ê´€ë¦¬ ë°©ì‹)"""
    opts = Options()
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--window-size=1920,1080")
    opts.add_argument("user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/120 Safari/537.36")
    
    try:
        driver = webdriver.Chrome(options=opts)
    except Exception as e:
        print(f"âŒ WebDriver ìƒì„± ì˜¤ë¥˜: {e}")
        print("---")
        print("âš ï¸  Chrome ë¸Œë¼ìš°ì €ê°€ ìµœì‹  ë²„ì „ì´ ë§ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        print("âš ï¸  'pip install --upgrade selenium'ì„ ì‹¤í–‰í•´ ì…€ë ˆë‹ˆì›€ì„ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œí•´ ë³´ì„¸ìš”.")
        return None
    return driver

def get_news_links(driver, stock_code, max_count):
    """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í˜ì´ì§€ì—ì„œ ê°œë³„ ë‰´ìŠ¤ ë§í¬ì™€ ì œëª© ì¶”ì¶œ"""
    # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ URLë¡œ ë³€ê²½
    LIST_URL = f"https://finance.naver.com/item/news_news.naver?code={stock_code}&page=1"

    driver.get(LIST_URL)
    print(f"ğŸ“„ ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í˜ì´ì§€ ë¡œë”© ì¤‘... ({stock_code})")

    try:
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        time.sleep(3)
    except Exception as e:
        print(f"âš ï¸  í˜ì´ì§€ ë¡œë“œ ê²½ê³ : {e}")

    soup = BeautifulSoup(driver.page_source, "html.parser")
    links_with_titles = []

    # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ëª©ë¡ í…Œì´ë¸”ì—ì„œ ì¶”ì¶œ
    news_table = soup.select("table.type5 tr")

    for row in news_table:
        # ì œëª©ê³¼ ë§í¬ê°€ ìˆëŠ” a íƒœê·¸ ì°¾ê¸°
        link_elem = row.select_one("td.title a")
        if not link_elem:
            continue

        href = link_elem.get("href")
        if not href:
            continue

        # ë„¤ì´ë²„ ë‰´ìŠ¤ ì ˆëŒ€ URL ìƒì„±
        if href.startswith("/"):
            full_url = "https://finance.naver.com" + href
        else:
            full_url = href

        title = link_elem.get_text(strip=True)

        # ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
        if len(title) < 10:
            continue

        item_tuple = (full_url, title)
        if full_url not in [link[0] for link in links_with_titles]:
            links_with_titles.append(item_tuple)
            print(f"    âœ… {len(links_with_titles)}. {title[:50]}... | {full_url[:60]}...")
            if max_count and len(links_with_titles) >= max_count:
                break

    return links_with_titles

def extract_article_content(driver, url, list_title="ì œëª© ì—†ìŒ"):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©, ë‚ ì§œ, ë³¸ë¬¸ ë“± ì¶”ì¶œ"""
    try:
        print(f"    ğŸŒ í˜ì´ì§€ ì ‘ì†: {url[:60]}...")
        driver.get(url)
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        time.sleep(2)

        soup = BeautifulSoup(driver.page_source, "html.parser")

        # ì œëª© ì¶”ì¶œ
        title = list_title
        if not title or title == "ì œëª© ì—†ìŒ" or len(title) < 5:
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ì œëª© êµ¬ì¡°
            title_elem = soup.select_one("#articleTitle, h2.media_end_head_headline, h3.font1")
            if title_elem:
                title = title_elem.get_text(strip=True)

        # ë³¸ë¬¸ ì¶”ì¶œ
        content_parts = []

        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì˜ì—­
        article_body = soup.select_one("#articleBodyContents, #newsEndContents, #articeBody")
        if article_body:
            print(f"    ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ë°œê²¬")
            # script, style íƒœê·¸ ì œê±°
            for tag in article_body.select("script, style, .ad, .relation"):
                tag.decompose()

            paragraphs = article_body.find_all(["p", "div"], recursive=False)
            if paragraphs:
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if text and len(text) > 20 and not text.startswith("//"):
                        content_parts.append(text)
            else:
                # paragraphê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸
                text = article_body.get_text(strip=True)
                lines = [line.strip() for line in text.split('\n') if line.strip()]
                content_parts = [line for line in lines if len(line) > 20 and not line.startswith("//")]

        # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì¼ë°˜ article íƒœê·¸ ì‹œë„
        if not content_parts:
            article_bodies = soup.select("article")
            if article_bodies:
                print(f"    ğŸ“° article íƒœê·¸ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„")
                for article in article_bodies:
                    for tag in article.select("script, style"):
                        tag.decompose()
                    paragraphs = article.find_all(["p"])
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if text and len(text) > 20:
                            content_parts.append(text)

        content = "\n\n".join(content_parts)

        # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
        press = None
        press_elem = soup.select_one(".press_logo img, .media_end_head_top_logo img, .press")
        if press_elem:
            press = press_elem.get("alt") or press_elem.get("title")
        if not press:
            og_site = soup.select_one("meta[property='og:article:author']")
            if og_site:
                press = og_site.get("content")

        # ë‚ ì§œ ì¶”ì¶œ
        date = None
        date_elem = soup.select_one(".t11, .article_info span, .article_date, time")
        if date_elem:
            date = date_elem.get_text(strip=True)

        if not title and not content:
            return {"url": url, "error": "ì œëª©ê³¼ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

        return {
            "url": url,
            "title": title or "ì œëª© ì—†ìŒ",
            "press": press or "ì¶œì²˜ ë¯¸ìƒ",
            "date": date or "ë‚ ì§œ ë¯¸ìƒ",
            "content": content or "ë³¸ë¬¸ ì—†ìŒ",
            "content_length": len(content)
        }

    except Exception as e:
        return {"url": url, "error": f"íŒŒì‹± ì‹¤íŒ¨: {str(e)}"}