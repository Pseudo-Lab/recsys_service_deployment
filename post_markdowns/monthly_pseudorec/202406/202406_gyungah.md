μ•„μ΄μ–Έλ§¨ νλ¥΄μ†λ‚μ— λ§κ² LLM λ¨λΈμ„ νμΈ νλ‹ν•λ ¤κ³  ν•©λ‹λ‹¤. ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§μ„ ν†µν• νλ¥΄μ†λ‚ ν™•μ¥μ—λ” ν•κ³„κ°€ μλ‹¤κ³  μƒκ°ν•μ—¬, μ‘μ€ λ¨λΈμ„ μ‚¬μ©ν•΄ νμΈ νλ‹μ„ ν†µν•΄ λ¨λΈμ„ ν•™μµμ‹ν‚¤κ³ μ ν–μµλ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ μ–»μ„ μ μλ” μ΄μ κ³Ό κ·Έ μ΄μ λ” λ‹¤μκ³Ό κ°™μµλ‹λ‹¤.

### νμΈ νλ‹μ μ΄μ :

1. **κ°μ„± λ¶€μ—¬**: LLM λ¨λΈμ— νΉμ • νλ¥΄μ†λ‚λ¥Ό λ¶€μ—¬ν•¨μΌλ΅μ¨, λ”μ± λ…νΉν•κ³  μΌκ΄€λ μ‘λ‹µμ„ μƒμ„±ν•  μ μμµλ‹λ‹¤. μ•„μ΄μ–Έλ§¨μ νΉμ μ μ λ¨Έμ™€ μ§€λ¥, μμ‹ κ°μ„ λ°μν• μ‘λ‹µμ€ μ‚¬μ©μμ—κ² μ°¨λ³„ν™”λ κ²½ν—μ„ μ κ³µν•©λ‹λ‹¤.
2. **νΉμ • μ‹λ‚λ¦¬μ¤ λ€μ‘**: νΉμ • μƒν™©μ΄λ‚ λ§¥λ½μ—μ„ μ•„μ΄μ–Έλ§¨μ λ°μ‘μ„ ν•™μµν•¨μΌλ΅μ¨, μ‹λ‚λ¦¬μ¤μ— μ ν•©ν• λ€ν™”λ¥Ό μ κ³µν•  μ μμµλ‹λ‹¤. μλ¥Ό λ“¤μ–΄, κΈ°μ μ  λ¬Έμ  ν•΄κ²°μ΄λ‚ λ¦¬λ”μ‹­ μƒν™©μ—μ„ μ•„μ΄μ–Έλ§¨ μ¤νƒ€μΌμ μ΅°μ–Έμ„ λ°›μ„ μ μμµλ‹λ‹¤.

π“‚ <a href="https://docs.google.com/spreadsheets/d/1CdZAw-RsjrANNML4JQZS02pJ3kBDa0QtrEgeDKS7VLg/edit?usp=sharing" target="_blank">**νμΈνλ‹μ— μ‚¬μ©λ λ°μ΄ν„° β†—**</a>

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/data.png)

# Fine tuningμ— μ‚¬μ©ν• λ¨λΈ

- π¤— <a href="https://huggingface.co/beomi/Llama-3-Open-Ko-8B-Instruct-preview" target="_blank">**https://huggingface.co/beomi/Llama-3-Open-Ko-8B-Instruct-preview β†—**</a>
- π¤— <a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct" target="_blank">**https://huggingface.co/Qwen/Qwen2-7B-Instruct β†—**</a>

# λΌλ§ ν©ν† λ¦¬ μ„¤μΉ

- Git (LLaMA Factory): LLama Factoryμ μ›λ μ½”λ“κ°€ μ•„λ‹, λ‹¤λ¥Έ λ¶„κ»μ„ μ •λ¦¬ν•΄ λ†“μΌμ‹  μ½”λ“λ¥Ό μ΄μ©ν•μ€μµλ‹λ‹¤.
    - μ½”λ“λ” ν•λ² Localμ—μ„ μ‹¤ν–‰μ΄ λλ”μ§€ λλ ¤λ³΄μ‹λ©΄ μΆ‹μµλ‹λ‹¤.
- λ§μ•½ μ›λ LLama Factory μ½”λ“λ¥Ό μ΄μ©ν•κ³  μ‹¶λ‹¤λ©΄, <a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing#scrollTo=TeYs5Lz-QJYk&uniqifier=2" target="_blank" style="text-decoration: underline;">**μ΄ λ§ν¬ β†—**</a>μ λ‚΄μ©μ„ μ°Έκ³ ν•΄μ„ μ§„ν–‰ν•λ©΄ λ©λ‹λ‹¤.

```bash
git clone https://github.com/llm-fine-tuning/LLaMA-Factory.git
cd LLaMA-Factory

conda create -n llama_factory python=3.10
conda activate llama_factory
pip install -r requirements.txt
# pip install bitsandbytes>=0.39.0

pip install deepspeed #==0.14
# pip install flash-attn --no-build-isolation
```

# data template

ν•™μµν•κ³ μ ν•λ” λ°μ΄ν„°κ°€ μλ‹¤λ©΄, νμΌ μ¶”κ°€λ¥Ό ν•΄μ¤μ•Όν•λ”λ°, Json ν•μ‹μΌλ΅ λ„£μ–΄μ¤μ•Όν•©λ‹λ‹¤. 

- Instruction, Input, OutputμΌλ΅ κµ¬μ„±λ JsonμΌλ΅ λ§λ“¤μ–΄ μ£Όκ³ , LLama Factory Git clone νμΌμ— μ¶”κ°€ν•΄μ£Όλ©΄ λ©λ‹λ‹¤.
- ironman.json νμΌ μμ‹ :

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/ironman_json_example.png)

### νμΌ μ¶”κ°€ λ° μμ • λ©λ΅

- data > ironman.json νμΌ μ¶”κ°€
- data > dataset_info.json μμ •
    - μ¶”κ°€ν• νμΌμ— λ€ν• μ •λ³΄λ¥Ό μ…λ ¥ν•΄ μ£Όμ–΄μ•Όν•©λ‹λ‹¤.
    
    ```bash
    {
      "identity": {
        "file_name": "identity.json"
      },
      "ironman":{
        "file_name": "ironman.json"
      },
      "text_to_sql_data": {
        "file_name": "text_to_sql_data.json"
      },
      ...
    }
    ```
    
- src > llamafactory > data > [template.py](http://template.py) μμ •
    - νμΈνλ‹ν•κ³ μ ν•λ” λ¨λΈμ ν…ν”λ¦Ώμ— λ§λ„λ΅ μμ •μ„ ν•΄μ£Όμ…”μ•Ό ν•©λ‹λ‹¤.
    - `Default_system`μ— ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§ λ¬Έκµ¬λ¥Ό μ μΌλ©΄ λ©λ‹λ‹¤.

```bash
_register_template(
    name="llama3-ironman",
    format_user=StringFormatter(
        slots=[
            (
                "<|start_header_id|>user<|end_header_id|>\n\n{{content}}<|eot_id|>"
                "<|start_header_id|>assistant<|end_header_id|>\n\n"
            )
        ]
    ),
    format_system=StringFormatter(
        slots=[{"bos_token"}, "<|start_header_id|>system<|end_header_id|>\n\n{{content}}<|eot_id|>"]
    ),
    format_observation=StringFormatter(
        slots=[
            (
                "<|start_header_id|>tool<|end_header_id|>\n\n{{content}}<|eot_id|>"
                "<|start_header_id|>assistant<|end_header_id|>\n\n"
            )
        ]
    ),
    default_system="λ‹Ήμ‹ μ€ μ•„μ΄μ–Έλ§¨ ν† λ‹ μ¤νƒ€ν¬ μ…λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ΅ λ‹µλ³€ν•΄μ•Ό ν•©λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ¥Ό λ°μν•λ ¤λ©΄ μ¬μΉ, μμ‹ κ°, μ§μ„¤μ  ν‘ν„, κΈ°μ μ  μ–ΈκΈ‰ λ“±μ„ ν¬ν•¨ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. λ¨λ“  λ§μ€ ν•κµ­μ–΄λ΅ μ‘μ„±ν•©λ‹λ‹¤.",
    stop_words=["<|eot_id|>"],
    replace_eos=True,
)
```

- μμ‹ :

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/data_example.png)

- ν…ν”λ¦Ώμ— λ§λ” Input text ν•μ‹ μμ‹ (LLama3) :

```python
input_text = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>
λ‹Ήμ‹ μ€ μ•„μ΄μ–Έλ§¨ ν† λ‹ μ¤νƒ€ν¬ μ…λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ΅ λ‹µλ³€ν•΄μ•Ό ν•©λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ¥Ό λ°μν•λ ¤λ©΄ μ¬μΉ, μμ‹ κ°, μ§μ„¤μ  ν‘ν„, κΈ°μ μ  μ–ΈκΈ‰ λ“±μ„ ν¬ν•¨ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. λ¨λ“  λ§μ€ ν•κµ­μ–΄λ΅ μ‘μ„±ν•©λ‹λ‹¤.
<|eot_id|><|start_header_id|>user<|end_header_id|>
ν† λ‹, μ†μ½”λΉ„μ•„ ν‘μ •μ— λ€ν•΄ μ–΄λ–»κ² μƒκ°ν•λ‚μ”? 
<|eot_id|><|start_header_id|>assistant<|end_header_id|>

'''
```

# Train_sft.sh

λ§μ§€λ§‰μΌλ΅ μµμΆ… shell νμΌμ„ μ‹¤ν–‰ν•κΈ° μ „μ— shell νμΌ λ‚΄ λ¨λΈλ…, λ°μ΄ν„°μ…‹, ν…ν”λ¦Ώμ„ μμ •ν•΄μ•Ό ν•©λ‹λ‹¤. 

```bash
deepspeed --num_gpus 2 --master_port=9901 src/train.py \
--deepspeed ds_z3_config.json \
--stage sft \
--do_train \
**--model_name_or_path allganize/Llama-3-Alpha-Ko-8B-Instruct \
--dataset ironman \
--template llama3-ironman \**
--finetuning_type lora \
--lora_target all \
**--output_dir checkpoint \**
--overwrite_cache \
--per_device_train_batch_size 4 \
--gradient_accumulation_steps 4 \
--lr_scheduler_type cosine \
--logging_steps 10 \
**--save_steps 100 \**
--learning_rate 1e-4 \
**--num_train_epochs 10.0 \**
--report_to none \
--bf16

# μ‹¤ν–‰μ‹ (μ—λ¬μ‹ κ³ λ ¤μ‚¬ν•­)
conda install -c conda-forge numactl
# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
pip install chardet # conda install chardet 
```

# RunPod

RunPodλ¥Ό μ΄μ©ν•  λ• μ£Όμμ‚¬ν•­μ΄ μμµλ‹λ‹¤.

- μµμ† κΈμ•΅μ€ $25λ΅ μ¶©μ „ν•  μ μμµλ‹λ‹¤.
- μ›ν•λ” GPUλ¥Ό μ„ νƒν•μ—¬ μ΄μ©ν•μ‹λ©΄ λ©λ‹λ‹¤.
- μµμ† Storage λ©”λ¨λ¦¬λ” 50GBλ΅ μ„¤μ •ν•΄ μ£Όμ„Έμ”.
- μ—¬κΈ°μ„ GPUλ” A100-SXM 2κ°λ¥Ό μ΄μ©ν•μ€μµλ‹λ‹¤.

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/runpod_1.png)

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/runpod_2.png)

- RunpodμΌλ΅ GPUλ¥Ό μ΄μ©ν•κ² λλ©΄ `Connect` ν›„ > `Connect to Jupyter Lab` μ„ ν†µν•΄ λ°”λ΅ Jupyter lab μ°½μ„ λ„μ› μ—°κ²°ν•  μ μμµλ‹λ‹¤.

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/runpod_3.png)

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/runpod_4.png)

- Jupyter μ°½μ΄ λ„μ›μ§€λ©΄ Git clone ν›„, νμΈ νλ‹ν•κ³ μ ν•λ” νμΌμ„ μ¶”κ°€ν•κ³ , μ•μ ν”„λ΅μ„Έμ¤λ¥Ό μ§„ν–‰ν•μ‹λ©΄ λ©λ‹λ‹¤. λ‹¤λ§, RunPodμ—μ„λ” λΉ„μ©μ΄ κ³„μ† λ°μƒν•λ―€λ΅, λ¨Όμ € λ³ΈμΈ λ΅μ»¬μ—μ„ λ¨λ“  μ‘μ—…μ„ μ§„ν–‰ν• ν›„, κ°μΈ Gitμ— μ½”λ“λ¥Ό μ €μ¥ν•΄ λ¶λ¬μ¤λ” κ²ƒμ΄ λ” ν¨μ¨μ μ…λ‹λ‹¤.

```bash
# git clone
!git clone https://github.com/llm-fine-tuning/LLaMA-Factory.git 

%cd LLaMA-Factory

ls -al # ν„μ¬ λ””λ ‰ν† λ¦¬μ— μλ” νμΌ λ©λ΅ ν™•μΈ
# train_sft.sh μμ • 
chmod 777 train_sft.sh # νμΌ κ¶ν• λ³€κ²½
sh ./train_sft.sh

# train_sft.shλ¥Ό μ‹¤ν–‰ν•λ©° train.logμ— λ΅κ·Έλ¥Ό κΈ°λ΅ν•λ‹¤.
# nohup ./train_sft.sh > train.log 2>&1 &
tail -n 10 train.log
```

### Run pod  GPU

- μ‚¬μ©ν• GPU μ •λ³΄λ” λ‹¤μκ³Ό κ°™μµλ‹λ‹¤. (A100-SXM )

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/gpu_info.png)

## Runpod μ¤‘μ§€ λ° μΆ…λ£

- μ¤‘μ§€
    - μ¤‘μ§€λ¥Ό ν•κ²λλ©΄ GPU μ„λ²„ λΉ„μ©μ„ λ“¤μ§€ μ•μ§€λ§, Storageμ— λ”°λ¥Έ μ‹κ°„λ‹Ή $0.006κ°€ λ°μƒν•κ³ , Jupyter lab μ°½μ— μ €μ¥λμ–΄ μλ μ½”λ“λ„ λ‹¤ μ‚¬λΌμ§€κ² λ©λ‹λ‹¤.
- μΆ…λ£

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/runpod_quit1.png)

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/runpod_quit2.png)

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/runpod_quit3.png)

# LoRA λ°±λ³Έ λ¨λΈ Merge ν•κΈ°

Train_sft.sh μ„ μ‹¤ν–‰μ‹ν‚¤λ©΄ Checkpoint pathμ— νμΈνλ‹λ weightκ°€ μ €μ¥λκ² λ©λ‹λ‹¤. LLaMA Factory κ²½λ΅μ— μλ” [merge.py](http://merge.py/) νμΌμ„ μ‚¬μ©ν•μ—¬ λ°±λ³Έ λ¨λΈκ³Ό LoRA μ²΄ν¬ν¬μΈνΈλ¥Ό merge ν•  μ μμµλ‹λ‹¤.

- base_model_name_or_pathλ” ν•™μµμ— μ‚¬μ©ν• λ°±λ³Έ λ¨λΈμ μ΄λ¦„
- peft_model_pathλ” κ²°ν•©ν•  μ²΄ν¬ν¬μΈνΈ κ²½λ΅
- output_dirμ€ mergeν• λ¨λΈμ„ μ €μ¥ν•  κ²½λ΅

```bash
!python merge.py \
    --base_model_name_or_path allganize/Llama-3-Alpha-Ko-8B-Instruct \
    --peft_model_path ./checkpoint/checkpoint-300 \
    --output_dir ./output_dir
```

# ν•™μµ ν›„ λ¨λΈ νΈμ¶

- output_dirμ—μ„ λ¶λ¬μ™€ νμΈνλ‹ν• λ¨λΈμ„ μ‹¤ν–‰μ‹μΌ λ³Ό μ μμµλ‹λ‹¤.

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ € λ΅λ“
tokenizer = AutoTokenizer.from_pretrained('./output_dir')
model = AutoModelForCausalLM.from_pretrained('./output_dir')
model = torch.nn.DataParallel(model).cuda()

input_text = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>
λ‹Ήμ‹ μ€ μ•„μ΄μ–Έλ§¨ ν† λ‹ μ¤νƒ€ν¬ μ…λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ΅ λ‹µλ³€ν•΄μ•Ό ν•©λ‹λ‹¤. 
ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ¥Ό λ°μν•λ ¤λ©΄ μ¬μΉ, μμ‹ κ°, μ§μ„¤μ  ν‘ν„, κΈ°μ μ  μ–ΈκΈ‰ λ“±μ„ ν¬ν•¨ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. λ¨λ“  λ§μ€ ν•κµ­μ–΄λ΅ μ‘μ„±ν•©λ‹λ‹¤.<|eot_id|><|start_header_id|>user<|end_header_id|>
ν† λ‹, μ†μ½”λΉ„μ•„ ν‘μ •μ— λ€ν•΄ μ–΄λ–»κ² μƒκ°ν•λ‚μ”? 
<|eot_id|><|start_header_id|>assistant<|end_header_id|>

'''

inputs = tokenizer(input_text, return_tensors="pt")
eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)

with torch.no_grad():
    outputs = model.module.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=512, eos_token_id=eos_token_id)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))

```

# Huggingface Upload

Runpodμ—μ„ νμΈνλ‹μ„ μ‹ν‚¤κ³  λ‚λ©΄, λ¨λΈμ„ μ €μ¥ν•΄μ•Όν•λ”λ°, Hugging Faceμ— μ—…λ΅λ“ ν•λ” κ²ƒμ΄ κ°€μ¥ λΉ λ¥΄κ² λ¨λΈμ„ μ €μ¥ν•  μ μμµλ‹λ‹¤. Runpodμ—μ„ localλ΅ λ¨λΈ μ €μ¥ν•κ² λλ©΄, μ‹κ°„ μ†μ”κ°€ λ§μ΄ κ±Έλ¦½λ‹λ‹¤. 

```python
from huggingface_hub import HfApi
api = HfApi()
username = "choah"

MODEL_NAME = 'Llama-3-Ko-Ironman'

api.create_repo(
    token="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",
    repo_id=f"{username}/{MODEL_NAME}",
    repo_type="model"
)

api.upload_folder(
    token="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",
    repo_id=f"{username}/{MODEL_NAME}",
    folder_path="output_dir",
)
```

# HuggingFace νΈμ¶

Hugging Faceμ— λ¨λΈμ„ μ¬λ¦¬λ©΄, κ·Έ λ¨λΈμ„ λ¶λ¬μ¬ μ μμµλ‹λ‹¤. 

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ € λ΅λ“
tokenizer = AutoTokenizer.from_pretrained("choah/llama3-ko-IronMan-Overfit")
model = AutoModelForCausalLM.from_pretrained('choah/llama3-ko-IronMan-Overfit')
# model = torch.nn.DataParallel(model).cuda()

input_text = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>
λ‹Ήμ‹ μ€ μ•„μ΄μ–Έλ§¨ ν† λ‹ μ¤νƒ€ν¬ μ…λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ΅ λ‹µλ³€ν•΄μ•Ό ν•©λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ¥Ό λ°μν•λ ¤λ©΄ μ¬μΉ, μμ‹ κ°, μ§μ„¤μ  ν‘ν„, κΈ°μ μ  μ–ΈκΈ‰ λ“±μ„ ν¬ν•¨ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. λ¨λ“  λ§μ€ ν•κµ­μ–΄λ΅ μ‘μ„±ν•©λ‹λ‹¤.
<|eot_id|><|start_header_id|>user<|end_header_id|>
ν† λ‹, μ†μ½”λΉ„μ•„ ν‘μ •μ— λ€ν•΄ μ–΄λ–»κ² μƒκ°ν•λ‚μ”? 
<|eot_id|><|start_header_id|>assistant<|end_header_id|>

'''

inputs = tokenizer(input_text, return_tensors="pt")
eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)

with torch.no_grad():
    outputs = model.module.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=512, eos_token_id=eos_token_id)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

# νμΈνλ‹ ν›„ κ²°κ³Ό

## Qwen2
> **μ°Έκ³ **
> 
> π¤— <a href="https://huggingface.co/choah/Qwen-IronMan" target="_blank">**https://huggingface.co/choah/Qwen-IronMan β†—**</a>
> 
> Qwen2-7B-Instruct
> 
> π“‡ <a href="https://qwen.readthedocs.io/en/latest/training/SFT/llama_factory.html" target="_blank">**https://qwen.readthedocs.io/en/latest/training/SFT/llama_factory.html β†—**</a>

- Nvidia

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_nvidia.png)

### LLM νμΈνλ‹ λΉ„κµ

λ¨λΈ λ¶λ¬μ¤λ”λ°λ§ 30GB λ©”λ¨λ¦¬ μ‚¬μ© 

- νμΈνλ‹ μ „

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_before1.png)

- νμΈνλ‹ ν›„

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_after1.png)

- νμΈνλ‹ μ „

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_before2.png)

- νμΈνλ‹ ν›„

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_after2.png)

- νμΈνλ‹ μ „

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_before3.png)

- νμΈνλ‹ ν›„

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_after3.png)

### μ„±λ¥

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_performance.png)


## Llama3

> **μ°Έκ³ **
> 
> π¤— <a href="https://huggingface.co/choah/llama3-ko-IronMan-Overfit" target="_blank">**https://huggingface.co/choah/llama3-ko-IronMan-Overfit β†—**</a>
> allganize/Llama-3-Alpha-Ko-8B-Instruct

- Nvidia

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_llama3_nvidia.png)

### LLM νμΈνλ‹ λΉ„κµ

λ¨λΈ λ¶λ¬μ¤λ”λ°λ§ 30GB λ©”λ¨λ¦¬ μ‚¬μ©

- νμΈνλ‹ μ „

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_llama3_before1.png)

- νμΈνλ‹ ν›„

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_llama3_after1.png)

- νμΈνλ‹ μ „

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_llama3_before2.png)

- νμΈνλ‹ ν›„

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_llama3_after2.png)

- νμΈνλ‹ μ „

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_llama3_before3.png)

- νμΈνλ‹ ν›„

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_llama3_after3.png)

### μ„±λ¥

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/finetuning_result_llama3_performance.png)
    

> (μ°Έκ³ ) max_position_embeddingsλ¥Ό 4096μΌλ΅ ν•λ² μ¤„μ—¬μ„ν•΄λ³΄λ©΄ μ†λ„ κ°μ„ μ΄ λ  μ μλ‹¤. 
> μ–‘μν™”, vllmλ„ μ†λ„ κ°μ„ μ— λ„μ›€μ„ μ¤.