μ•„μ΄μ–Έλ§¨ νλ¥΄μ†λ‚μ— λ§κ² LLM λ¨λΈμ„ νμΈ νλ‹ν•λ ¤κ³  ν•©λ‹λ‹¤. ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§μ„ ν†µν• νλ¥΄μ†λ‚ ν™•μ¥μ—λ” ν•κ³„κ°€ μλ‹¤κ³  μƒκ°ν•μ—¬, μ‘μ€ λ¨λΈμ„ μ‚¬μ©ν•΄ νμΈ νλ‹μ„ ν†µν•΄ λ¨λΈμ„ ν•™μµμ‹ν‚¤κ³ μ ν–μµλ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ μ–»μ„ μ μλ” μ΄μ κ³Ό κ·Έ μ΄μ λ” λ‹¤μκ³Ό κ°™μµλ‹λ‹¤.

### νμΈ νλ‹μ μ΄μ :

1. **κ°μ„± λ¶€μ—¬**: LLM λ¨λΈμ— νΉμ • νλ¥΄μ†λ‚λ¥Ό λ¶€μ—¬ν•¨μΌλ΅μ¨, λ”μ± λ…νΉν•κ³  μΌκ΄€λ μ‘λ‹µμ„ μƒμ„±ν•  μ μμµλ‹λ‹¤. μ•„μ΄μ–Έλ§¨μ νΉμ μ μ λ¨Έμ™€ μ§€λ¥, μμ‹ κ°μ„ λ°μν• μ‘λ‹µμ€ μ‚¬μ©μμ—κ² μ°¨λ³„ν™”λ κ²½ν—μ„ μ κ³µν•©λ‹λ‹¤.
2. **νΉμ • μ‹λ‚λ¦¬μ¤ λ€μ‘**: νΉμ • μƒν™©μ΄λ‚ λ§¥λ½μ—μ„ μ•„μ΄μ–Έλ§¨μ λ°μ‘μ„ ν•™μµν•¨μΌλ΅μ¨, μ‹λ‚λ¦¬μ¤μ— μ ν•©ν• λ€ν™”λ¥Ό μ κ³µν•  μ μμµλ‹λ‹¤. μλ¥Ό λ“¤μ–΄, κΈ°μ μ  λ¬Έμ  ν•΄κ²°μ΄λ‚ λ¦¬λ”μ‹­ μƒν™©μ—μ„ μ•„μ΄μ–Έλ§¨ μ¤νƒ€μΌμ μ΅°μ–Έμ„ λ°›μ„ μ μμµλ‹λ‹¤.

π“‚ <a href="https://docs.google.com/spreadsheets/d/1CdZAw-RsjrANNML4JQZS02pJ3kBDa0QtrEgeDKS7VLg/edit?usp=sharing" target="_blank">**νμΈνλ‹μ— μ‚¬μ©λ λ°μ΄ν„° β†—**</a>

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/data.png)

# Fine tuningμ— μ‚¬μ©ν• λ¨λΈ

- π¤— <a href="https://huggingface.co/beomi/Llama-3-Open-Ko-8B-Instruct-preview" target="_blank">**https://huggingface.co/beomi/Llama-3-Open-Ko-8B-Instruct-preview β†—**</a>
- π¤— <a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct" target="_blank">**https://huggingface.co/Qwen/Qwen2-7B-Instruct β†—**</a>

# λΌλ§ ν©ν† λ¦¬ μ„¤μΉ

- Git (LLaMA Factory): LLama Factoryμ μ›λ μ½”λ“κ°€ μ•„λ‹, λ‹¤λ¥Έ λ¶„κ»μ„ μ •λ¦¬ν•΄ λ†“μΌμ‹  μ½”λ“λ¥Ό μ΄μ©ν•μ€μµλ‹λ‹¤.
    - μ½”λ“λ” ν•λ² Localμ—μ„ μ‹¤ν–‰μ΄ λλ”μ§€ λλ ¤λ³΄μ‹λ©΄ μΆ‹μµλ‹λ‹¤.
- λ§μ•½ μ›λ LLama Factory μ½”λ“λ¥Ό μ΄μ©ν•κ³  μ‹¶λ‹¤λ©΄, <a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing#scrollTo=TeYs5Lz-QJYk&uniqifier=2" target="_blank" style="text-decoration: underline;">**μ΄ λ§ν¬ β†—**</a>μ λ‚΄μ©μ„ μ°Έκ³ ν•΄μ„ μ§„ν–‰ν•λ©΄ λ©λ‹λ‹¤.

```bash
git clone https://github.com/llm-fine-tuning/LLaMA-Factory.git
cd LLaMA-Factory

conda create -n llama_factory python=3.10
conda activate llama_factory
pip install -r requirements.txt
# pip install bitsandbytes>=0.39.0

pip install deepspeed #==0.14
# pip install flash-attn --no-build-isolation
```

# data template

ν•™μµν•κ³ μ ν•λ” λ°μ΄ν„°κ°€ μλ‹¤λ©΄, νμΌ μ¶”κ°€λ¥Ό ν•΄μ¤μ•Όν•λ”λ°, Json ν•μ‹μΌλ΅ λ„£μ–΄μ¤μ•Όν•©λ‹λ‹¤. 

- Instruction, Input, OutputμΌλ΅ κµ¬μ„±λ JsonμΌλ΅ λ§λ“¤μ–΄ μ£Όκ³ , LLama Factory Git clone νμΌμ— μ¶”κ°€ν•΄μ£Όλ©΄ λ©λ‹λ‹¤.
- ironman.json νμΌ μμ‹ :

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/ironman_json_example.png)

### νμΌ μ¶”κ°€ λ° μμ • λ©λ΅

- data > ironman.json νμΌ μ¶”κ°€
- data > dataset_info.json μμ •
    - μ¶”κ°€ν• νμΌμ— λ€ν• μ •λ³΄λ¥Ό μ…λ ¥ν•΄ μ£Όμ–΄μ•Όν•©λ‹λ‹¤.
    
    ```bash
    {
      "identity": {
        "file_name": "identity.json"
      },
      "ironman":{
        "file_name": "ironman.json"
      },
      "text_to_sql_data": {
        "file_name": "text_to_sql_data.json"
      },
      ...
    }
    ```
    
- src > llamafactory > data > [template.py](http://template.py) μμ •
    - νμΈνλ‹ν•κ³ μ ν•λ” λ¨λΈμ ν…ν”λ¦Ώμ— λ§λ„λ΅ μμ •μ„ ν•΄μ£Όμ…”μ•Ό ν•©λ‹λ‹¤.
    - `Default_system`μ— ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§ λ¬Έκµ¬λ¥Ό μ μΌλ©΄ λ©λ‹λ‹¤.

```bash
_register_template(
    name="llama3-ironman",
    format_user=StringFormatter(
        slots=[
            (
                "<|start_header_id|>user<|end_header_id|>\n\n{{content}}<|eot_id|>"
                "<|start_header_id|>assistant<|end_header_id|>\n\n"
            )
        ]
    ),
    format_system=StringFormatter(
        slots=[{"bos_token"}, "<|start_header_id|>system<|end_header_id|>\n\n{{content}}<|eot_id|>"]
    ),
    format_observation=StringFormatter(
        slots=[
            (
                "<|start_header_id|>tool<|end_header_id|>\n\n{{content}}<|eot_id|>"
                "<|start_header_id|>assistant<|end_header_id|>\n\n"
            )
        ]
    ),
    default_system="λ‹Ήμ‹ μ€ μ•„μ΄μ–Έλ§¨ ν† λ‹ μ¤νƒ€ν¬ μ…λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ΅ λ‹µλ³€ν•΄μ•Ό ν•©λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ¥Ό λ°μν•λ ¤λ©΄ μ¬μΉ, μμ‹ κ°, μ§μ„¤μ  ν‘ν„, κΈ°μ μ  μ–ΈκΈ‰ λ“±μ„ ν¬ν•¨ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. λ¨λ“  λ§μ€ ν•κµ­μ–΄λ΅ μ‘μ„±ν•©λ‹λ‹¤.",
    stop_words=["<|eot_id|>"],
    replace_eos=True,
)
```

- μμ‹ :

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/data_example.png)

- ν…ν”λ¦Ώμ— λ§λ” Input text ν•μ‹ μμ‹ (LLama3) :

```python
input_text = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>
λ‹Ήμ‹ μ€ μ•„μ΄μ–Έλ§¨ ν† λ‹ μ¤νƒ€ν¬ μ…λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ΅ λ‹µλ³€ν•΄μ•Ό ν•©λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ¥Ό λ°μν•λ ¤λ©΄ μ¬μΉ, μμ‹ κ°, μ§μ„¤μ  ν‘ν„, κΈ°μ μ  μ–ΈκΈ‰ λ“±μ„ ν¬ν•¨ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. λ¨λ“  λ§μ€ ν•κµ­μ–΄λ΅ μ‘μ„±ν•©λ‹λ‹¤.
<|eot_id|><|start_header_id|>user<|end_header_id|>
ν† λ‹, μ†μ½”λΉ„μ•„ ν‘μ •μ— λ€ν•΄ μ–΄λ–»κ² μƒκ°ν•λ‚μ”? 
<|eot_id|><|start_header_id|>assistant<|end_header_id|>

'''
```

# Train_sft.sh

λ§μ§€λ§‰μΌλ΅ μµμΆ… shell νμΌμ„ μ‹¤ν–‰ν•κΈ° μ „μ— shell νμΌ λ‚΄ λ¨λΈλ…, λ°μ΄ν„°μ…‹, ν…ν”λ¦Ώμ„ μμ •ν•΄μ•Ό ν•©λ‹λ‹¤. 

```bash
deepspeed --num_gpus 2 --master_port=9901 src/train.py \
--deepspeed ds_z3_config.json \
--stage sft \
--do_train \
**--model_name_or_path allganize/Llama-3-Alpha-Ko-8B-Instruct \
--dataset ironman \
--template llama3-ironman \**
--finetuning_type lora \
--lora_target all \
**--output_dir checkpoint \**
--overwrite_cache \
--per_device_train_batch_size 4 \
--gradient_accumulation_steps 4 \
--lr_scheduler_type cosine \
--logging_steps 10 \
**--save_steps 100 \**
--learning_rate 1e-4 \
**--num_train_epochs 10.0 \**
--report_to none \
--bf16

# μ‹¤ν–‰μ‹ (μ—λ¬μ‹ κ³ λ ¤μ‚¬ν•­)
conda install -c conda-forge numactl
# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
pip install chardet # conda install chardet 
```

# RunPod

RunPodλ¥Ό μ΄μ©ν•  λ• μ£Όμμ‚¬ν•­μ΄ μμµλ‹λ‹¤.

- μµμ† κΈμ•΅μ€ $25λ΅ μ¶©μ „ν•  μ μμµλ‹λ‹¤.
- μ›ν•λ” GPUλ¥Ό μ„ νƒν•μ—¬ μ΄μ©ν•μ‹λ©΄ λ©λ‹λ‹¤.
- μµμ† Storage λ©”λ¨λ¦¬λ” 50GBλ΅ μ„¤μ •ν•΄ μ£Όμ„Έμ”.
- μ—¬κΈ°μ„ GPUλ” A100-SXM 2κ°λ¥Ό μ΄μ©ν•μ€μµλ‹λ‹¤.

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/runpod_1.png)

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/runpod_2.png)

- RunpodμΌλ΅ GPUλ¥Ό μ΄μ©ν•κ² λλ©΄ `Connect` ν›„ > `Connect to Jupyter Lab` μ„ ν†µν•΄ λ°”λ΅ Jupyter lab μ°½μ„ λ„μ› μ—°κ²°ν•  μ μμµλ‹λ‹¤.

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/runpod_3.png)

![Untitled](../../../static/img/monthly_pseudorec_202406/gyungah/runpod_4.png)

- Jupyter μ°½μ΄ λ„μ›μ§€λ©΄ Git clone ν›„, νμΈ νλ‹ν•κ³ μ ν•λ” νμΌμ„ μ¶”κ°€ν•κ³ , μ•μ ν”„λ΅μ„Έμ¤λ¥Ό μ§„ν–‰ν•μ‹λ©΄ λ©λ‹λ‹¤. λ‹¤λ§, RunPodμ—μ„λ” λΉ„μ©μ΄ κ³„μ† λ°μƒν•λ―€λ΅, λ¨Όμ € λ³ΈμΈ λ΅μ»¬μ—μ„ λ¨λ“  μ‘μ—…μ„ μ§„ν–‰ν• ν›„, κ°μΈ Gitμ— μ½”λ“λ¥Ό μ €μ¥ν•΄ λ¶λ¬μ¤λ” κ²ƒμ΄ λ” ν¨μ¨μ μ…λ‹λ‹¤.

```bash
# git clone
!git clone https://github.com/llm-fine-tuning/LLaMA-Factory.git 

%cd LLaMA-Factory

ls -al # ν„μ¬ λ””λ ‰ν† λ¦¬μ— μλ” νμΌ λ©λ΅ ν™•μΈ
# train_sft.sh μμ • 
chmod 777 train_sft.sh # νμΌ κ¶ν• λ³€κ²½
sh ./train_sft.sh

# train_sft.shλ¥Ό μ‹¤ν–‰ν•λ©° train.logμ— λ΅κ·Έλ¥Ό κΈ°λ΅ν•λ‹¤.
# nohup ./train_sft.sh > train.log 2>&1 &
tail -n 10 train.log
```

### Run pod  GPU

- μ‚¬μ©ν• GPU μ •λ³΄λ” λ‹¤μκ³Ό κ°™μµλ‹λ‹¤. (A100-SXM )

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/000a2619-de24-4cfa-9347-ee3b6d56f5d6/Untitled.png)

## Runpod μ¤‘μ§€ λ° μΆ…λ£

- μ¤‘μ§€
    - μ¤‘μ§€λ¥Ό ν•κ²λλ©΄ GPU μ„λ²„ λΉ„μ©μ„ λ“¤μ§€ μ•μ§€λ§, Storageμ— λ”°λ¥Έ μ‹κ°„λ‹Ή $0.006κ°€ λ°μƒν•κ³ , Jupyter lab μ°½μ— μ €μ¥λμ–΄ μλ μ½”λ“λ„ λ‹¤ μ‚¬λΌμ§€κ² λ©λ‹λ‹¤.
- μΆ…λ£

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/eb7da0a8-7e69-4423-8f1e-5f783d84db3b/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/c28968a6-4710-43e2-bc9e-da09ced1da41/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/3b43cbe0-fef4-42af-b164-58291deecd66/Untitled.png)

# LoRA λ°±λ³Έ λ¨λΈ Merge ν•κΈ°

Train_sft.sh μ„ μ‹¤ν–‰μ‹ν‚¤λ©΄ Checkpoint pathμ— νμΈνλ‹λ weightκ°€ μ €μ¥λκ² λ©λ‹λ‹¤. LLaMA Factory κ²½λ΅μ— μλ” [merge.py](http://merge.py/) νμΌμ„ μ‚¬μ©ν•μ—¬ λ°±λ³Έ λ¨λΈκ³Ό LoRA μ²΄ν¬ν¬μΈνΈλ¥Ό merge ν•  μ μμµλ‹λ‹¤.

- base_model_name_or_pathλ” ν•™μµμ— μ‚¬μ©ν• λ°±λ³Έ λ¨λΈμ μ΄λ¦„
- peft_model_pathλ” κ²°ν•©ν•  μ²΄ν¬ν¬μΈνΈ κ²½λ΅
- output_dirμ€ mergeν• λ¨λΈμ„ μ €μ¥ν•  κ²½λ΅

```bash
!python merge.py \
    --base_model_name_or_path allganize/Llama-3-Alpha-Ko-8B-Instruct \
    --peft_model_path ./checkpoint/checkpoint-300 \
    --output_dir ./output_dir
```

# ν•™μµ ν›„ λ¨λΈ νΈμ¶

- output_dirμ—μ„ λ¶λ¬μ™€ νμΈνλ‹ν• λ¨λΈμ„ μ‹¤ν–‰μ‹μΌ λ³Ό μ μμµλ‹λ‹¤.

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ € λ΅λ“
tokenizer = AutoTokenizer.from_pretrained('./output_dir')
model = AutoModelForCausalLM.from_pretrained('./output_dir')
model = torch.nn.DataParallel(model).cuda()

input_text = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>
λ‹Ήμ‹ μ€ μ•„μ΄μ–Έλ§¨ ν† λ‹ μ¤νƒ€ν¬ μ…λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ΅ λ‹µλ³€ν•΄μ•Ό ν•©λ‹λ‹¤. 
ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ¥Ό λ°μν•λ ¤λ©΄ μ¬μΉ, μμ‹ κ°, μ§μ„¤μ  ν‘ν„, κΈ°μ μ  μ–ΈκΈ‰ λ“±μ„ ν¬ν•¨ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. λ¨λ“  λ§μ€ ν•κµ­μ–΄λ΅ μ‘μ„±ν•©λ‹λ‹¤.<|eot_id|><|start_header_id|>user<|end_header_id|>
ν† λ‹, μ†μ½”λΉ„μ•„ ν‘μ •μ— λ€ν•΄ μ–΄λ–»κ² μƒκ°ν•λ‚μ”? 
<|eot_id|><|start_header_id|>assistant<|end_header_id|>

'''

inputs = tokenizer(input_text, return_tensors="pt")
eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)

with torch.no_grad():
    outputs = model.module.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=512, eos_token_id=eos_token_id)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))

```

# Huggingface Upload

Runpodμ—μ„ νμΈνλ‹μ„ μ‹ν‚¤κ³  λ‚λ©΄, λ¨λΈμ„ μ €μ¥ν•΄μ•Όν•λ”λ°, Hugging Faceμ— μ—…λ΅λ“ ν•λ” κ²ƒμ΄ κ°€μ¥ λΉ λ¥΄κ² λ¨λΈμ„ μ €μ¥ν•  μ μμµλ‹λ‹¤. Runpodμ—μ„ localλ΅ λ¨λΈ μ €μ¥ν•κ² λλ©΄, μ‹κ°„ μ†μ”κ°€ λ§μ΄ κ±Έλ¦½λ‹λ‹¤. 

```python
from huggingface_hub import HfApi
api = HfApi()
username = "choah"

MODEL_NAME = 'Llama-3-Ko-Ironman'

api.create_repo(
    token="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",
    repo_id=f"{username}/{MODEL_NAME}",
    repo_type="model"
)

api.upload_folder(
    token="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",
    repo_id=f"{username}/{MODEL_NAME}",
    folder_path="output_dir",
)
```

# HuggingFace νΈμ¶

Hugging Faceμ— λ¨λΈμ„ μ¬λ¦¬λ©΄, κ·Έ λ¨λΈμ„ λ¶λ¬μ¬ μ μμµλ‹λ‹¤. 

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ € λ΅λ“
tokenizer = AutoTokenizer.from_pretrained("choah/llama3-ko-IronMan-Overfit")
model = AutoModelForCausalLM.from_pretrained('choah/llama3-ko-IronMan-Overfit')
# model = torch.nn.DataParallel(model).cuda()

input_text = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>
λ‹Ήμ‹ μ€ μ•„μ΄μ–Έλ§¨ ν† λ‹ μ¤νƒ€ν¬ μ…λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ΅ λ‹µλ³€ν•΄μ•Ό ν•©λ‹λ‹¤. ν† λ‹ μ¤νƒ€ν¬μ λ§ν¬λ¥Ό λ°μν•λ ¤λ©΄ μ¬μΉ, μμ‹ κ°, μ§μ„¤μ  ν‘ν„, κΈ°μ μ  μ–ΈκΈ‰ λ“±μ„ ν¬ν•¨ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. λ¨λ“  λ§μ€ ν•κµ­μ–΄λ΅ μ‘μ„±ν•©λ‹λ‹¤.
<|eot_id|><|start_header_id|>user<|end_header_id|>
ν† λ‹, μ†μ½”λΉ„μ•„ ν‘μ •μ— λ€ν•΄ μ–΄λ–»κ² μƒκ°ν•λ‚μ”? 
<|eot_id|><|start_header_id|>assistant<|end_header_id|>

'''

inputs = tokenizer(input_text, return_tensors="pt")
eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)

with torch.no_grad():
    outputs = model.module.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=512, eos_token_id=eos_token_id)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

# νμΈνλ‹ ν›„ κ²°κ³Ό

## Qwen2

- μ°Έκ³ 
    - https://huggingface.co/choah/Qwen-IronMan
    - Qwen2-7B-Instruct
    - https://qwen.readthedocs.io/en/latest/training/SFT/llama_factory.html

- Nvidia

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/000a2619-de24-4cfa-9347-ee3b6d56f5d6/Untitled.png)

### LLM νμΈνλ‹ λΉ„κµ

λ¨λΈ λ¶λ¬μ¤λ”λ°λ§ 30GB λ©”λ¨λ¦¬ μ‚¬μ© 

- νμΈνλ‹ μ „

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/6f75148c-0001-4f25-af95-2ee55f31f7f6/Untitled.png)

- νμΈνλ‹ ν›„

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/e176d971-1b43-4d7b-beb0-21fa987b1e08/Untitled.png)

- νμΈνλ‹ μ „

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/63010b7a-9cb6-4efe-a92d-de36473396c6/Untitled.png)

- νμΈνλ‹ ν›„

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/d4e3a4ec-74d3-4d89-a122-cf783750d682/2c281590-b324-4fe1-9c1e-6b7a193f06d3.png)

- νμΈνλ‹ μ „

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/95504356-5bd3-4794-8397-6e6ddbe14ceb/Untitled.png)

- νμΈνλ‹ ν›„

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/f46700c8-a281-451c-9af0-67684fbb1fa3/Untitled.png)

### μ„±λ¥

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/a70f5d45-959d-4925-9732-d6055de86250/Untitled.png)

- table
    
    Loss	Grad Norm	Learning Rate	Epoch
    2.166	1.00933480	9.993582535855263e-05	0.16
    1.6936	0.73289640	9.942341621640558e-05	0.48
    1.6215	0.82311741	9.897649706262473e-05	0.65
    1.6579	0.83676695	9.840385594331022e-05	0.81
    1.6023	1.04322338	9.770696282000244e-05	0.97
    1.4832	1.14594707	9.688760660735402e-05	1.13
    1.4482	1.35963062	9.594789058101153e-05	1.29
    1.4598	1.34016395	9.489022697853709e-05	1.45
    1.3651	1.53846825	9.371733080722911e-05	1.61
    1.3772	1.59791824	9.243221287473756e-05	1.77
    1.3074	1.51776745	9.103817206036382e-05	1.94
    1.2448	1.57799817	8.953878684688493e-05	2.10
    1.1363	1.89884352	8.793790613463955e-05	2.26
    1.1789	2.02771319	8.6239639361456e-05	2.42
    1.1466	1.87221225	8.444834595378434e-05	2.58
    1.1396	2.12810930	8.256862413611113e-05	2.74
    1.1495	2.21997633	8.060529912738315e-05	2.90
    1.0244	2.03679361	7.856341075473962e-05	3.06
    0.9197	2.57839350	7.644820051634812e-05	3.23
    0.9699	2.43997156	7.201970757788172e-05	3.55
    0.8743	2.51407475	6.971779275566593e-05	3.71
    0.938	2.64692886	6.736526264224101e-05	3.87
    0.8774	2.67930874	6.496815614866791e-05	4.03
    0.7118	2.84707941	6.253262661293604e-05	4.19
    0.744	2.79032188	6.006492600443301e-05	4.35
    0.6975	2.99523683	5.757138887522884e-05	4.52
    0.7256	3.01447854	5.505841609937161e-05	4.68
    0.7244	2.80243886	5.2532458441935636e-05	4.84
    0.7536	2.90106577	5e-05	5.00
    0.5438	2.74322781	4.746754155806437e-05	5.16
    0.557	2.95665736	4.49415839006284e-05	5.32
    0.5797	3.23495723	4.2428611124771184e-05	5.48
    0.5378	3.33052616	3.993507399556699e-05	5.65
    0.5732	3.36924427	3.746737338706397e-05	5.81
    0.5729	3.29053834	3.5031843851332104e-05	5.97
    0.4577	3.44033963	2.573490187344596e-05	6.61
    0.4186	3.37789145	2.3551799483651894e-05	6.77
    0.4258	3.18310049	2.1436589245260376e-05	6.94
    0.3812	2.57653466	1.9394700872616855e-05	7.10
    0.3224	3.06026295	1.7431375863888898e-05	7.26
    0.3546	2.89718737	1.555165404621567e-05	7.42 
    

## Llama3

- μ°Έκ³ 
    - https://huggingface.co/choah/llama3-ko-IronMan-Overfit
    - allganize/Llama-3-Alpha-Ko-8B-Instruct

- Nvidia

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/ce433904-f9df-45e5-b652-853d88a1c47f/Untitled.png)

### LLM νμΈνλ‹ λΉ„κµ

λ¨λΈ λ¶λ¬μ¤λ”λ°λ§ 30GB λ©”λ¨λ¦¬ μ‚¬μ©

- νμΈνλ‹ μ „

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/3f809a1c-ec21-4429-a777-90f1546fe795/Untitled.png)

- νμΈνλ‹ ν›„

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/69f55afd-17a1-432e-9c5f-e309482561aa/Untitled.png)

- νμΈνλ‹ μ „

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/97dbd798-b496-42cb-b972-9ccc3291854a/23e9aeb9-5926-42ab-abb1-eaf96b463c34.png)

- νμΈνλ‹ ν›„

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/7aea2de6-0009-4760-a1af-b067a1f46e1c/28623cae-9aed-4c49-9149-9ca172fbb3a1.png)

- νμΈνλ‹ μ „

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/e749602c-9dc6-4740-90ab-0f25f3d483df/Untitled.png)

- νμΈνλ‹ ν›„

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/95751f9f-8d3c-4ef8-b1b3-03c3114df756/Untitled.png)

### μ„±λ¥

![output (1).png](https://prod-files-secure.s3.us-west-2.amazonaws.com/592a16fe-1e93-4755-8cf9-0097ed9a5c31/447d108e-6b5d-4454-b2bb-3b61751c6e47/output_(1).png)

- Table
    
    epoch	loss	grad_norm	learning_rate
    0.32	1.8516	0.9893	9.9743e-05
    0.48	1.7194	1.0966	9.9423e-05
    0.65	1.6538	1.0797	9.8976e-05
    0.81	1.7002	1.1063	9.8403e-05
    0.97	1.6502	1.2959	9.7707e-05
    1.13	1.5127	1.3902	9.6888e-05
    1.29	1.4679	1.4633	9.5948e-05
    1.45	1.4664	1.5495	9.4890e-05
    1.61	1.3865	1.6647	9.3717e-05
    1.77	1.4108	1.7921	9.2432e-05
    1.94	1.3228	1.6999	9.1038e-05
    2.10	1.2517	1.7224	8.9539e-05
    2.26	1.1289	2.0544	8.7938e-05
    2.42	1.1755	2.0649	8.6240e-05
    2.58	1.1376	2.0037	8.4448e-05
    2.74	1.1241	2.1767	8.2569e-05
    2.90	1.1466	2.3999	8.0605e-05
    3.06	1.0036	2.1389	7.8563e-05
    3.23	0.8710	2.5385	7.6448e-05
    3.39	0.8777	2.4703	7.4265e-05
    3.55	0.9372	2.6776	7.2020e-05
    3.71	0.8425	2.5992	6.9718e-05
    3.87	0.9001	2.9833	6.7365e-05
    4.03	0.8295	2.8457	6.4968e-05
    4.19	0.6524	3.0004	6.2533e-05
    4.35	0.6901	2.8679	6.0065e-05
    4.52	0.6537	3.0197	5.7571e-05
    4.68	0.6644	3.0981	5.5058e-05
    4.84	0.6692	3.0046	5.2532e-05
    5.00	0.6847	3.2592	5.0000e-05
    5.16	0.4656	2.9259	4.7468e-05
    5.32	0.4957	3.1546	4.4942e-05
    5.48	0.5156	3.5975	0.0000424286111
    5.65	0.4668	3.6223	0.0000399350740
    5.81	0.5157	3.5422	0.0000374673734
    5.97	0.4953	3.3117	0.0000350318439
    6.13	0.4021	3.3538	0.0000326347374
    6.29	0.3895	2.7937	0.0000302822072
    6.45	0.3254	3.1592	0.0000279802924
    6.61	0.3705	3.5733	0.0000257349019
    6.77	0.3569	3.0589	0.0000235517995
    6.94	0.3581	3.4002	0.0000214365892
    7.10	0.3052	2.8063	0.0000193947009
    7.26	0.2438	3.1322	0.0000174313759
    7.42	0.2794	2.8185	0.0000155516540
    7.58	0.2598	2.5592	0.0000137603606
    7.74	0.2547	2.8546	0.0000120620939
    7.90	0.2762	3.1193	0.0000104612132
    8.06	0.2268	2.1699	0.0000089618280
    8.23	0.1895	2.4984	0.0000075677871
    8.39	0.2187	2.5002	0.0000062826692
    

> (μ°Έκ³ ) max_position_embeddingsλ¥Ό 4096μΌλ΅ ν•λ² μ¤„μ—¬μ„ν•΄λ³΄λ©΄ μ†λ„ κ°μ„ μ΄ λ  μ μλ‹¤. 
μ–‘μν™”, vllmλ„ μ†λ„ κ°μ„ μ— λ„μ›€μ„ μ¤.
>